---
title: "Practical Machine Learning Course Project"
author: "Andrew Kaiser-Tedesco"
date: "June 19, 2018"
output: html_document
---

**Background**

In this project, my goal will be to use data from accelerometers worn by people working out on the belt, forearm, arm, and dumbell of 6 participants. I will use this data to train a model that predicts the exercise (as classified in the "classe" variable). 

**Load and Preprocess**

First, I load and clean the data.

``` {r}
library(caret)
library(ggplot2)
set.seed(1007)
pmlData <- read.csv("pml-training.csv")
pmlQuiz <- read.csv("pml-testing.csv")
```

The first 7 variables include meta data, which will not assist with analysis. The last column is the categorical class, which is the dependent variable in our model. 
We'll now preprocess by removing all the variables with near zero variance. 

``` {r}

# Remove near zero variance
nzv <- nearZeroVar(pmlData)
working <- pmlData[,-nzv]

# Remove mostly NAs
mostlyNA <- sapply(working, function(x) mean(is.na(x))) > 0.95
working <- working[,mostlyNA == FALSE]

#Remove variables that cannot theoretically be part of the model:
# (x, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new window, number window). Or the first 7 columns
working <- working[,-(1:7)]
quiz <- pmlQuiz[,-(1:7)]

```

Now we'll partition the data into a training and testing set.

``` {r}
inTrain <- createDataPartition(y=working$classe, p = 0.7, list = FALSE)
training <- working[inTrain,]
testing <- working[-inTrain,]
fullset <- working

dim(training)
dim(testing)

```

**Model Building**

To start, I use a random forest model with 3-fold cross validation to tune the optimization parameters. Then I use the model I get to predict "classe" on the testing set to come up with an estimate of out-of-sample error. 

```{r, cache = TRUE}
rfControl <- trainControl(method="cv", number=3, verboseIter=FALSE)

rf <- train(classe ~ ., data = training, method = "rf", trControl=rfControl)

rf$finalModel

predrf <- predict(rf, newdata = testing)
confusionMatrix(testing$classe, predrf)$overall


```

The confusion matrix reveals that out-of-sample accuracy is ~99.2%, an extremely good result. The predicted out of sample error is then ~0.8%. This result is good enough for our purposes, so I'll choose to stick with a random forest model.


**Retraining the model & Make final predictions**

Since we'll be moving on to testing in the validation set (here called the Quiz data), we'll retrain the whole model using the full set of data for maximum accuracy. The final output provides the exercise predictions for our new 20 data inputs.

``` {r, cache = TRUE}
# Retrain the model on the fullset
rfFinal <- train(classe ~ ., data = fullset, method = "rf", trControl=rfControl)

#Predict the quiz set and reformat as characters
predQuiz <- predict(rfFinal, newdata = quiz)
print(predQuiz)

```
